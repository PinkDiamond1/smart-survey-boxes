{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation and Verfication of Data Processing Steps\n",
    "This notebook serves two main purposes:\n",
    "1. Describe the data processing steps done to the data\n",
    "2. Validate/audit the data at each step to ensure that they are no errors in the processing. Because of this\n",
    "I use the file **sms-verification.xml** for all the processing in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the pypower package and others\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "# add our main processing modules to python path, so we can easily import them\n",
    "sys.path.append('/Users/dmatekenya/Google-Drive/worldbank/smart-survey-boxes/code')\n",
    "import pandas as pd\n",
    "import operator\n",
    "\n",
    "# pypower\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from data_processing import data_processing_utils as util\n",
    "from data_processing import data_processing_engine as dp\n",
    "\n",
    "# Plotting\n",
    "from IPython.display import Image\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# some global variables\n",
    "TIME_FORMAT = '%Y-%m-%d %H:%M:%S'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naming Conventions and Important folders\n",
    "The root folder where all work outputs are saved is **OneDrive/William Hutchins Seitz-01.Electricity_monitoring/**. Selected utputs (e.g., model evaluation plots) from the data processing\n",
    "I do can be found in **./outputs_from_dunstan_data_processing**. All the data are saved in **./01.data**. Within this data folder, there are a couple of important subfolders as follows:\n",
    "\n",
    "* processed-sms\n",
    "   * sms_observed.csv: contains both valid and invalid sms events. Definition of valid can be found in the later sections of this document\n",
    "   * sms_observed_valid.csv: Has only valid events.\n",
    "   * sms_rect_hr.csv: Rectangularised valid events with missing event labeled\n",
    "   * sms_rect_hr_imputed.csv: Same as above but with missing events imputed.\n",
    "* raw-sms-backup: Daily backup of sms.xml\n",
    "* imputation-verification: Derived datasets for checking sanity of the imputation are saved here\n",
    "* spatial-data: I save all geograpgic datasets here\n",
    "* tableau-inputs: If there is need to create specific tableau inputs, they are kept here\n",
    " \n",
    "\n",
    "They are two other source folders to take note of:\n",
    "- **SharedGoogDrive/SMSBuckupRestore :** As everyone is aware, this contains the sms.xml file which is updated everyday\n",
    "- **SharedGoogDrive/smart-survey-boxes :** I shared this Google-Drive folder. It contains all the work I do. Specifically, the code I use is in the **./code**. This is the code which is being used to process the data, although the data is save in William OneDrive folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up environment\n",
    "data_folder, xml_folder, outputs_folder = None, None, None\n",
    "if sys.platform == 'darwin':\n",
    "    data_folder = os.path.abspath('/Users/dmatekenya/Google-Drive/worldbank/smart-survey-boxes/data')\n",
    "    xml_folder = os.path.abspath(\"/Users/dmatekenya/Google-Drive/SMSBuckupRestore/\")\n",
    "    outputs_folder = os.path.abspath('/Users/dmatekenya/Google-Drive/worldbank/smart-survey-boxes/outputs')\n",
    "\n",
    "if sys.platform == 'win32':\n",
    "    # windows-onedrive\n",
    "    data_folder = os.path.normpath(os.path.abspath(\"C:/Users/wb344850/WBG/William Hutchins Seitz - 01.data/\"))\n",
    "    xml_folder = os.path.normpath(os.path.abspath(\"C:/Users/wb344850/Google-Drive/SMSBuckupRestore/\"))\n",
    "    outputs_folder = os.path.normpath(os.path.abspath(\"C:/Users/wb344850/Google-Drive/worldbank/smart-survey-boxes/outputs\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw sms data back up\n",
    "The *sms.xml* is is converted to *sms.csv* everyday. This is set up using \n",
    "\n",
    "windows scheduler on my windows bank computer. There is no processing done\n",
    "\n",
    "to the data in this process. The sms files are saved with a date suffix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== DATA BACK-UP   ==========================\n",
    "# Initiate a data processor object\n",
    "data_processor_params = {'data_dir': data_folder, 'process_type': 'backup', 'verification_mode': True,\n",
    "                             'debug_mode': True, 'xml_dir': xml_folder, 'outputs_dir': outputs_folder,\n",
    "                             'box_dist_ver': 14}\n",
    "\n",
    "data_processor = dp.DataProcessor(**data_processor_params)\n",
    "data_processor.process_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify that the data is indeed intact\n",
    "We check the data which is saved with today date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We get latest file in the raw-sms-backup directory\n",
    "backup_dir = data_processor.raw_data_dir  # this will be replaced by \n",
    "data_files = [f for f in os.listdir(backup_dir) if os.path.isfile(os.path.join(backup_dir, f))]\n",
    "file_time = {}\n",
    "\n",
    "for f in data_files:\n",
    "    if f[-3:] == 'csv':\n",
    "        full_path = os.path.join(backup_dir, f)\n",
    "        last_update = os.path.getmtime(full_path)\n",
    "        file_time[last_update] = full_path\n",
    "        \n",
    "sorted_x = sorted(file_time.items(), key=operator.itemgetter(0), reverse=True)\n",
    "latest_csv_file = sorted_x[0][1]\n",
    "\n",
    "# Now lets check the csv file to make sure its the same as the xml file\n",
    "df = pd.read_csv(latest_csv_file, low_memory=False)\n",
    "print('Number of events in this sms.xml: {:,}'.format(df.shape[0]))\n",
    "print('Event attributes...')\n",
    "cols = list(df.columns)\n",
    "print(cols[:8])\n",
    "print(cols[8:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process sms data\n",
    "This is the first processing step. During this step, the main processing done are as follows:\n",
    "- Fixing dates\n",
    "- Setting attributes for events\n",
    "- etc.\n",
    "\n",
    "Each event is given many attributes as shown in figure below. After all the processing, \n",
    "here are the outputs data files:\n",
    "1. sms_observed.csv: contains all sms events including the invalid ones. Invalid events are those which satisfy the following condition: test message, no device id in message body, impssoible to generate message type from message or event occured before start date of data collection.\n",
    "\n",
    "2. sms_observed_valid.csv: keep only the valid events\n",
    "\n",
    "3. sms_rect_hr.csv: Based on only valid events, create rectangular dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('====================')\n",
    "print(' EVENT ATTRIBUTES')\n",
    "print('====================')\n",
    "Image(filename='/Users/dmatekenya/Google-Drive/worldbank/smart-survey-boxes/' +\n",
    "      'outputs/data-processing-documentation/event_attr.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the processed data outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location processed data files\n",
    "processed_sms =  data_processor.processed_data_dir\n",
    "observed = os.path.join(processed_sms, 'sms_observed.csv')\n",
    "observed_valid = os.path.join(processed_sms, 'sms_observed_valid.csv')\n",
    "rect =  os.path.join(processed_sms, 'sms_rect_hr.csv')\n",
    "rect_imp = os.path.join(processed_sms, 'sms_rect_hr_imputed.csv')\n",
    "outage_sum_comp = os.path.join(processed_sms, 'outage_summary_comparison.csv')\n",
    "\n",
    "# Read into data frame\n",
    "df_observed = pd.read_csv(observed)\n",
    "df_observed_valid = pd.read_csv(observed_valid)\n",
    "df_rect = pd.read_csv(rect)\n",
    "df_imp = pd.read_csv(rect_imp)\n",
    "df_sum_comp = pd.read_csv(outage_sum_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_observed_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rect.power_state.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A few things to note about sms_rect_hr\n",
    "As can be seen above, [99] is for invalid and should be dropped from analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rect[['box_id','event_type_str', 'power_state', 'data_source', 'str_datetime_sent','str_datetime_sent_hr' ]][:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if number of events after processing matches with original number of events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of events in observed.csv\n",
    "print ('Number of events in observed.csv: {:,}'.format(df_observed.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine invalid events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_observed.valid.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_test_events(str):\n",
    "    if 'test' in str:\n",
    "        return 1\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** How many of the invalid are test events?**\n",
    "\n",
    "We see that the number of test events matches with what is in the stata data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_observed['test'] = df_observed['valid'].apply(lambda x: detect_test_events(x))\n",
    "df_observed['test'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='/Users/dmatekenya/Google-Drive/worldbank/smart-survey-boxes/' +\n",
    "      'outputs/data-processing-documentation/stata_test_events.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** How many sms with no device-id?**\n",
    "\n",
    "Again here, number of events with no id is same as in in stata code data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_no_id_events(str):\n",
    "    if 'no-device-id' in str:\n",
    "        return 1\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_observed['no_id'] = df_observed['valid'].apply(lambda x: detect_no_id_events(x))\n",
    "df_observed['no_id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** The dataset from here doesnt necessarily match the one generated from stata from here onwards due mainly to before-date-of-collection**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_before_start_of_collection(str):\n",
    "    if 'before-date-of-collection' in str:\n",
    "        return 1\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_observed['before_start'] = df_observed['valid'].apply(lambda x: detect_before_start_of_collection(x))\n",
    "df_observed['before_start'].value_counts()\n",
    "\n",
    "df_before_start = df_observed.groupby(['box_id','before_start']).count().reset_index()\n",
    "df_before_start = df_before_start[df_before_start.before_start == 1]\n",
    "df_before_start = df_before_start[['box_id', 'before_start','region']]\n",
    "df_before_start.rename(columns={'region': 'count'}, inplace=True)\n",
    "df_before_start.sort_values(by='count', ascending=False, inplace=True)\n",
    "print('='*70)\n",
    "print('Top 10 boxes with events which occurred before collection started')\n",
    "print('='*70)\n",
    "df_before_start[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine missing and invalid values\n",
    "A detailed description of how hours are flagged as being missing or not can be found in the documentation.\n",
    "A function which was used to flag events is shown below. As for the values:\n",
    "**power state: ** [1:power on, 0:power off, -1:missing, 99:invalid]. \n",
    "Given this coding scheme, all invalid values events are omitted when training model for imputation.\n",
    "Also, they should be omitted in any analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Image(filename='/Users/dmatekenya/Google-Drive/worldbank/smart-survey-boxes/' +\n",
    "      'outputs/data-processing-documentation/flagging_missing_values.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** The number of missing values can be seen below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rect.power_state.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** After imputation, they are no missing values. Invalid values are maintained for verification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imp.power_state.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imp.data_source.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How missing values are being flagged\n",
    "\n",
    "Illustrate the process of flagging missing values with a few boxes. We take a single box (id = 1301)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Select box-id 1301 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a single box objects to illustrate/verify the process of flagging missing events\n",
    "bx_1301 = df_observed_valid[df_observed_valid.box_id == 1301]\n",
    "bx_1301['datetime_sent'] = bx_1301.apply(lambda x: datetime.strptime(x['str_datetime_sent'], TIME_FORMAT), axis=1)\n",
    "bx_1301 = bx_1301.sort_values(by='datetime_sent')\n",
    "cols = ['box_id', 'datetime_sent', 'str_datetime_sent_hr', 'event_type_str',\n",
    "       'power_state', 'data_source']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Below I'm just replicating the processing I do in the automated procedure **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class for holding all characteristics of a box, note that box id = 1301\n",
    "time_format = '%Y-%m-%d %H:%M:%S'\n",
    "box_obj = util.Box(date_collection_started=bx_1301['date_collection_started'],box_id=1301, psu=bx_1301['psu'].iloc[0], \n",
    "              lon=bx_1301['lon'].iloc[0], urban_rural=bx_1301['urban_rural'].iloc[0], lat=bx_1301['lat'].iloc[0], \n",
    "              district=bx_1301['district'].iloc[0], region=bx_1301['region'].iloc[0])\n",
    "\n",
    "# Add events to this box, note that some event attributes are skipped here\n",
    "for index, item in bx_1301.iterrows():\n",
    "    # create an event object with core attributes\n",
    "    event = util.Event(datetime_sent_raw=item['str_datetime_sent'],\n",
    "                      message='None')\n",
    "    # set all important attributes\n",
    "    event.datetime_sent = datetime.strptime(item['str_datetime_sent'], time_format)\n",
    "    event.event_type_str = item['event_type_str']\n",
    "    event.valid = 'valid'\n",
    "    event.power_state = item['power_state']\n",
    "    event.datetime_sent_hr = datetime.strptime(item['str_datetime_sent_hr'], time_format)\n",
    "    \n",
    "    # indicate data source\n",
    "    event.data_source = 'observed_event'\n",
    "    box_obj.add_event(event)\n",
    "\n",
    "# Keep only columns we need\n",
    "bx_1301 = bx_1301[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** In order to judge how many *missing* values we should expect, lets add a time interval.\n",
    "This is the interval between 2 observed events, which is essentially what dtermines how many missing values we will have.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_interval_col(df = None, time_col = None, time_interval_col_name=None):\n",
    "    \"\"\"\n",
    "    Add time interval column to a df\n",
    "    \"\"\"\n",
    "    # check if time-col is string, change it to date\n",
    "    if isinstance(df[time_col].iloc[0], str):\n",
    "        df[time_col] = df[time_col].apply(lambda x: datetime.strptime(x, TIME_FORMAT ))\n",
    "        \n",
    "    df2 = df.set_index(time_col)\n",
    "    df2[time_col] = df2.index\n",
    "    \n",
    "    # sort by index and calculate time interval\n",
    "    df2.sort_values(by=time_col, inplace=True)\n",
    "    \n",
    "    df2['interval'] = (df2[time_col]-df2[time_col].shift()).fillna(0)\n",
    "    \n",
    "    # convert to hours\n",
    "    df2[time_interval_col_name] = df2['interval'].apply(lambda x: x.total_seconds()/3600)\n",
    "    \n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add time-interval column\n",
    "bx_1301 = add_time_interval_col(df = bx_1301, time_col='datetime_sent', time_interval_col_name='interval_hrs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Below is a summary of the interval variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bx_1301['interval_hrs'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Now, lets turn to the automated procedure, rectangularise the dataset and see \n",
    "how many missing values we get**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_obj.generate_hourly_events_based_on_power_state(after_event_threshold=13,invalid_threshold=72)\n",
    "box_metadata = box_obj.get_box_metadata(required_box_metadata_lst=['box_id','region', 'district', 'urban_rural','psu', 'lon', 'lat'])\n",
    "\n",
    "columns = ['box_id', 'datetime_sent', 'str_datetime_sent_hr', 'event_type_str', 'power_state', 'data_source']\n",
    "hr_events_metadata = []\n",
    "\n",
    "for eve in box_obj.hourly_events.values():\n",
    "    event_attr = eve.get_selected_event_metadata()\n",
    "    event_attr.update(box_metadata)\n",
    "    hr_events_metadata.append(event_attr)\n",
    "\n",
    "# create dataframe\n",
    "df = pd.DataFrame(hr_events_metadata)\n",
    "\n",
    "# sort by datetime_sent\n",
    "df.sort_values(by='datetime_sent', inplace=True)\n",
    "\n",
    "# reorder columns\n",
    "bx_1301_auto = df[columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Lets add time passed from observed event, compare with flagged missing values and see if its working as it should be.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add variable which will hold time_after_previous_event\n",
    "bx_1301_auto['time_after_prev'] = None\n",
    "\n",
    "# keep observed event date at whenever it aperas\n",
    "observed_event_time = bx_1301_auto['datetime_sent'].iloc[0]\n",
    "\n",
    "# Not a very neat way to handle this\n",
    "index_time_dict = {}\n",
    "\n",
    "for index, row in bx_1301_auto.iterrows():\n",
    "    if  row['data_source'] == 'observed_event':\n",
    "        observed_event_time = row['datetime_sent']\n",
    "    \n",
    "    #current event time\n",
    "    current_event_time = row['datetime_sent']\n",
    "    \n",
    "    # Time difference between current event and previous observed event (hrs)\n",
    "    time_interval = (current_event_time-observed_event_time).total_seconds()/3600\n",
    "    \n",
    "    # set 'time_after_prev'\n",
    "    index_time_dict[index] = time_interval\n",
    "\n",
    "bx_1301_auto['time_after_prev'] = bx_1301_auto.apply(lambda x: index_time_dict[x.name], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Take a look at the output')\n",
    "bx_1301_auto.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** We can see below that missing is only appearing in hours which are greater than 13 hrs after observed event**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bx_1301_auto[bx_1301_auto.power_state == -1][:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing statistics between imputed and unimputed datasets\n",
    "A few details about the process as below:\n",
    "- The variable being used is **daily average outage duration**.\n",
    "- This variable was computed for *sms_rect_hr.csv* [actual] and *sms_rect_hr_imputed.csv* [imputed]\n",
    "- The computation is done by first counting daily outage hours within a box to come up with \n",
    "*daily_outage_duration_actual.csv* and then averaging average across boxes for each day to get *outage_summary_comparison.csv* which has summaries for both **actual** and **imputed**\n",
    "- Results of comparing the **actual** vs. **imputed** are shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First 10 rows of the comparison ...\")\n",
    "df_sum_comp.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('======================================')\n",
    "print(' IMPUTED DATASET VS. ACTUAL DATASET')\n",
    "print('======================================')\n",
    "Image(filename='/Users/dmatekenya/Google-Drive/worldbank/smart-survey-boxes/' +\n",
    "      'outputs/data-processing-documentation/outage_summary_comparison.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarise_outage_counts_from_stata_powerout_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypespes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = os.path.join(data_folder, 'powerout.csv')\n",
    "# df = pd.read_csv(file)\n",
    "# df = df[df.POWERout == 1]  # keep only power_off events\n",
    "# df['date_sent'] = df['date_powerfailure'].apply(lambda x: datetime.strptime(x, '%d%b%Y'))\n",
    "# # df.rename(columns={'BoxID': 'box_id', 'POWERout':'power_off','dhms':'datetime_sent',\n",
    "# #                    'date_powerfailure_hour':'hour_sent'}, inplace=True)\n",
    "# # df = df[['box_id', 'power_off', 'date_sent', 'hour_sent', 'datetime_sent']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_sum = util.summarise_outage_counts_from_stata_powerout_file(powerout_file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_sent</th>\n",
       "      <th>hrs_power_off</th>\n",
       "      <th>hrs_power_off_10pm</th>\n",
       "      <th>box_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-11-26</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-11-26</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-11-27</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-11-27</td>\n",
       "      <td>24.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-11-28</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    date_sent  hrs_power_off  hrs_power_off_10pm  box_id\n",
       "0  2016-11-26            8.0                   0    1001\n",
       "1  2016-11-26            8.0                   5    1001\n",
       "2  2016-11-27           24.0                   0    1001\n",
       "3  2016-11-27           24.0                   5    1001\n",
       "4  2016-11-28           17.0                   0    1001"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sum.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57989, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sum.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grp = util.calculate_daily_average_power_out(df=df_sum, var='hrs_power_off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>date_sent</th>\n",
       "      <th>avg_hrs_power_off</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-10-25</td>\n",
       "      <td>11.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2016-10-26</td>\n",
       "      <td>11.470588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2016-10-27</td>\n",
       "      <td>13.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2016-10-28</td>\n",
       "      <td>15.677419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2016-10-29</td>\n",
       "      <td>14.565217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   day   date_sent  avg_hrs_power_off\n",
       "0    1  2016-10-25          11.600000\n",
       "1    2  2016-10-26          11.470588\n",
       "2    3  2016-10-27          13.850000\n",
       "3    4  2016-10-28          15.677419\n",
       "4    5  2016-10-29          14.565217"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_grp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
